\section{Introduction and motivation - 1 pg}

When we think about the major challenges facing materials science, we are fundamentally faced with this idea of inversely designing materials.
That is, I decide that I want to create a material that behaves your sweat-wicking shirt under one condition, stiffens under another, and when given a particular stimulus can reconfigure its structure.
Currently, if I wanted to make a material like that for you, I'd naively take materials that have each of those properties and figure out how I could get them to work together.
Or, I'd look for novel materials that have properties close to those of the material I want to make.
We would call this designing the material.

This is inefficient.
In the inverse design problem, we take the properties we want and create the materials that will give us those properties.
Machine learning and materials science are coming together on the active front of this research.
However, being able to predict, even perfectly, what can be made from existing materials by definition limits us to the set of materials that currently exist.
This is a known challenge in materials science-- how do we probabilistically explore phase space outside of phase space where we have data?
While intriguing in its own right, that is not the topic of the thesis proposed here.

Instead, we might think about this from a fundamental physics point of view.
If we want to make complex materials that have embedded stimuli responses, or assemble into a specific target structure, we must give the building blocks of such complex materials some amount of direction.
We can think about this amount of direction as an amount of \textit{information}.

This is not to say that we are looking to have building blocks act as storage devices, as in \cite{Phillips_2014_SoftMatter}.
In that work, each building block is a cluster of multiple particles in whose arrangement can be stored a ``high density'' of information.

Similarly, in a recent proposal between our group and those of Marke Bathe (MIT), Mawgwi Bawendi (MIT), and Oleg Gang (Columbia), we proposed a biosynthetic, high-density storage structure composed of DNA nanocubes (Figure \ref{fig:semisynbio}).
Within these nanocubes, information could be stored in the different dies intercolated into the frame of the cube, into quantum dots placed into the frames, or even in the shape of the frames themselves.
If we them move a level higher, we can imagine storing additional information in the order of these nanocubes relative to one another.

However, using these and solutions like them for high-density storage requires us being able to write, read, and store information into these formats.
Fundamentally, these three challenges are predicated upon the ability to specifically place blocks where they need to go (``write'').
Current methods include sonic and laser tweezers (manual), specific DNA interactions (energetic), or incremental addition (kinetic).
How do we compare between these methods, though?

Here, I propose that the ability of building blocks to form a target structure can be distilled down into a concept of \textit{information}.

This is not a new concept.
In our group, we are comfortable with the concept that a target structure is the result of a minimization of free energy.
In systems devoid of inter-particle forces, this then reduces down to a maximization of entropy.

Statistical mechanical ``entropy'' shares its name with information ``entropy'' in communications theory.
While this directly came about because of the form similarity between the two, much energy since has been devoted to developing frameworks connecting the two.
Jaynes, in the 1950s, spent two long articles trying to reconcile the two. 
Books, and multiple articles, have been dedicated to explaining why these concepts are similar.

While much time has been spent developing the theory, very little time has been spent directly leveraging this concept for embedding information in systems governed by statistical mechanical ensembles-- such as colloidal-scale self-assembly.

Key line from Simons proposal: ``A coherent framework of thermodynamic and non-equilibrium processes seen through information theoretic eyes could lead to new theories for encoding information in matter-- which would allow for the design of novel materials and novel material behavioral control.''


New outline: 
\begin{itemize}
\item Self-assembly in materials science can lead to a variety of structures, complexity
\item The future of materials science relies upon inverse design
\item Inverse design requires understanding how the building blocks of a material inform its overall structure
\item Thinking more simply about tailored self-assembly... we want to direct the behavior of a system
\item We can think of this as giving the system some amount of information: binding preferences, kinetics, etc
\item Lots of work has gone into trying to understand how to measure and then most efficiently provide systems with that direction (will detail in the background section)
\item You know what does this really well, though? Proteins in nature
\item Proteins conformationally change while binding; lots of complexity
\item Here we use a simpler system of folding nets, which actually has much less complexity
\item The overarching challenge is: what is the most efficient way of assembling a given structure?
\item Such a question could motivate a career, so we will further limit this scope in the coming pages.
\end{itemize}

\section{Introduction and motivation - 1 pg}

When we think about the major challenges facing materials science, we are fundamentally faced with this idea of inversely designing materials.
That is, I decide that I want to create a material that behaves your sweat-wicking shirt under one condition, stiffens under another, and when given a particular stimulus can reconfigure its structure.
Currently, if I wanted to make a material like that for you, I'd naively take materials that have each of those properties and figure out how I could get them to work together.
Or, I'd look for novel materials that have properties close to those of the material I want to make.
We would call this designing the material.

This is inefficient.
In the inverse design problem, we take the properties we want and create the materials that will give us those properties.
Machine learning and materials science are coming together on the active front of this research.
However, being able to predict, even perfectly, what can be made from existing materials by definition limits us to the set of materials that currently exist.
This is a known challenge in materials science-- how do we probabilistically explore phase space outside of phase space where we have data?
While intriguing in its own right, that is not the topic of the thesis proposed here.

Instead, we might think about this from a fundamental physics point of view.
If we want to make complex materials that have embedded stimuli responses, or assemble into a specific target structure, we must give the building blocks of such complex materials some amount of direction.
We can think about this amount of direction as an amount of \textit{information}.

This is not to say that we are looking to have building blocks act as storage devices, as in \cite{Phillips_2014_SoftMatter}.
In that work, each building block is a cluster of multiple particles in whose arrangement can be stored a ``high density'' of information.

Similarly, in a recent proposal between our group and those of Marke Bathe (MIT), Mawgwi Bawendi (MIT), and Oleg Gang (Columbia), we proposed a biosynthetic, high-density storage structure composed of DNA nanocubes (Figure \ref{fig:semisynbio}).
Within these nanocubes, information could be stored in the different dies intercolated into the frame of the cube, into quantum dots placed into the frames, or even in the shape of the frames themselves.
If we them move a level higher, we can imagine storing additional information in the order of these nanocubes relative to one another.

However, using these and solutions like them for high-density storage requires us being able to write, read, and store information into these formats.
Fundamentally, these three challenges are predicated upon the ability to specifically place blocks where they need to go (``write'').
Current methods include sonic and laser tweezers (manual), specific DNA interactions (energetic), or incremental addition (kinetic).
How do we compare between these methods, though?

Here, I propose that the ability of building blocks to form a target structure can be distilled down into a concept of \textit{information}.

This is not a new concept.
In our group, we are comfortable with the concept that a target structure is the result of a minimization of free energy.
In systems devoid of inter-particle forces, this then reduces down to a maximization of entropy.

Statistical mechanical ``entropy'' shares its name with information ``entropy'' in communications theory.
While this directly came about because of the form similarity between the two, much energy since has been devoted to developing frameworks connecting the two.
Jaynes, in the 1950s, spent two long articles trying to reconcile the two. 
Books, and multiple articles, have been dedicated to explaining why these concepts are similar.

While much time has been spent developing the theory, very little time has been spent directly leveraging this concept for embedding information in systems governed by statistical mechanical ensembles-- such as colloidal-scale self-assembly.

Key line from Simons proposal: ``A coherent framework of thermodynamic and non-equilibrium processes seen through information theoretic eyes could lead to new theories for encoding information in matter-- which would allow for the design of novel materials and novel material behavioral control.''


\subsection{Prior thoughts on this matter}

Goal: Predictively and reliably encode information in materials systems. (I know this is too broad)

Problem statement:
\begin{itemize}
\item \textit{Engineering}: Designing functional, reconfigurable materials will require some method of storing information in a material-- we might call this ``memory''
\item \textit{Science}: Understanding how to ``store information'' in a material that embeds a response is a fundamental problem; further understanding the linkage between information-theoretic entropy and thermodynamic entropy concepts (accessible states, heat) could further our understand of dynamics of biological, etc processes
\end{itemize}


Ideas:
\begin{itemize}
\item Reconfigurable systems may be the key for unlocking adaptive material applications
\item At a particle level, we can think of this as building pluripotent building blocks that contain some response to a stimulus
\item At a system level, we can think of this as having metastable configurations in response to some stimulus
\item At an assembly-level, we can think of this as building blocks that can be engineered to form specifically-ordered arrays
\end{itemize}
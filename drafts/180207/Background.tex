\section{Literature Background - 2-3pg}

\subsection{What is information?}
% Good quora: https://math.stackexchange.com/questions/331103/intuitive-explanation-of-entropy

Let's first look at information in the context of communications theory.

The \textit{information}, $I$, we get from an event happening is given by:
\begin{equation}
I(p) = -\log{(p)}_b
\end{equation}

where $p$ is the probability of an event happening and $b$ is the base.
Base 2 is commonly used in information theory, and forms the unit of information.
For instance, the unit of base 2 information is a bit, base 3 are trits, base 10 are Harleys, and base $e$ are nats.

In 195X, Claude Shannon also extended this concept by introducing the concept of \textit{information entropy}.
In this context, entropy is the average (expected) amount of information gained from a given event.
Specifically, for an event with $n$ different outcomes this can be written as:
\begin{equation}
\text{Entropy} = \sum_{i=1}^{n}p_i\log{p_i}
\end{equation} 

For a discrete random variable $X$ with $p(x)$, the entropy can be written as:
\begin{equation}
H(X) = \sum_x p(x)\log{p(x)}
\end{equation}


Entropy does not range from 0 to 1.
The range is set based on the number of possible outcomes $n$, i.e. $-\leq\text{Entropy}\leq\log{(n)}$.
Entropy is equal to 0 (minimum entropy) when one of the probabilities is 1 and the rest are 0's.
Entropy is $\log{(n)}$ (maximum entropy) when all the probailities have equal values of 1/n.

In the case of designing specific outcomes for an event, then, we want to minimize the entropy along each leg of the pathway leading to an event.
Put another way, we want to maximize the probability that the event will proceed down the pathway we want it to.

However, the concept of ``information'' in this context is then counter-intuitive.
Information in communication refers to how likely an event is.
When a rare event happens, we gain more ``information'' from that event.
However, in the context of designing specific outcomes, we are not looking to read out bits of information once an event has happened.
We are looking to design the likelihood of an event occurring.

% Might want to include a ball-selection example here: http://www.csun.edu/~twang/595DM/Slides/Information%20&%20Entropy.pdf

In the words of MIT professor C\`{e}sar Hidalgo, ``It is hard for us humans to separate information from meaning because we cannot help interpreting messages.'' 
We face the same problem here-- by saying that a pathway has more information than another, we are implicitly saying that it is a rarer event than a lower-information pathway.

Counter-intuitively, in designing pathways for self-assembly, then, we are looking to design minimum-information pathways.
\textbf{However, in aligning with our intuition from self-assembly, this means we are looking to maximize the entropy of an assembly pathway.}

However, we can use the concept of \textit{mutual information} in defining how much information is stored in an interaction in an intuitive manner.
(See notes on the Brenner paper below.)
Mutual information $I(X;Y)$ is a global measure of interaction specificity in systems with many distinct species.
It quantifies how predictive the identity of a lock $x_i$ is to the identity of key $y_i$ found bound to it.

\subsubsection{How does this tie into statistical mechanics?}


\subsection{What is information, in the context of self-assembly?}

Let's first look at a paper from the Brenner group, the ``Information capacity of specific interactions' \cite{Huntley_2016_PNAS}.
Their main thesis is that specific binding interactions have energetics that allow binding to occur with measurable probability.
Thus, we can measure the relative information in different types of binding.
This is more in line with the communication theory view of information (rare events giving more information) than it is with the materials view of information, in which high information events imply high probability of a desired event happening.

Our group, and many others in the materials community, are looking to engineering materials to control their structures, behaviors, etc.
A common method of engineering these materials is by tailoring the interactions between their components through chemistry, shape, etc.
By understanding how much \textit{assembly information} can be contained in these interactions, we can:
\begin{enumerate}
\item Compare the efficacy of different types of interactions in delivering desired behavior(s)
\item Theoretically predict the efficacy of new types of interactions
\end{enumerate}

Let's take the example of a lock and key system.
\textcolor{red}{ADD SECTION ON BRENNER PAPER FROM LIT REVIEW LAST YEAR}


Any of Jacobs' papers that talk about this?: uses connectivity graphs


\subsection{Where have folks applied directed self-assembly? Why do we care about it?}
% i.e. what is the real-world problem we're actually trying to solve?

Glotzer, Kotov - self-assembly

Mirkin - experimental, DNA-directed

Kamien - kirigami

Glotzer, Desmaine - folding

Frenkel, Jacobs - pathway design

Wales - pathway designs, disconnectivity graphs


\subsection{Already proposed work: Semiconductor Synthetic Biology}

Include work done on the NSF grant proposal: Using DNA-mediated assembly to store information in nanoparticle arrays.

Specifically, this is an example of 1b): addressable complexity, then trying to engineer how to get the particles to where they should go in the most energetic and information/complexity-efficient manner possible.






